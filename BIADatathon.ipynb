{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d62c7a53-eea0-4292-9740-fec3f40c0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea5f0b0-65cd-4e0b-8a8c-c11a766aac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def cluster_visualise_scatter(df, num_clusters, col_name, dfname):\n",
    "    documents = df[col_name].dropna().tolist()\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_word_set)\n",
    "    idf_vector = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    kmeans=KMeans(n_clusters=num_clusters, random_state=42, n_init=10, max_iter=500)\n",
    "    clusters = kmeans.fit_predict(idf_vector.toarray())\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    #dimensionality reduction for visualisation\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(idf_vector.toarray())\n",
    "    \n",
    "    #Visualisation\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis', marker='o')\n",
    "    \n",
    "    #plot and label centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(centroids[:,0], centroids[:,1], c='red', marker='x', s=200, linewidths=3)\n",
    "    \n",
    "    #label centroids\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        plt.annotate(f'Cluster {i}', (centroid[0], centroid[1]), xytext=(10, 10),\n",
    "                     textcoords='offset points',\n",
    "                     fontweight='bold')\n",
    "    text = 't-SNE Visualization of similar {dfname} excerpts'\n",
    "    plt.title(text.format(dfname=dfname))\n",
    "    plt.show() \n",
    "\n",
    "    return clusters\n",
    "\n",
    "def visualize_wordcloud(df, num_clusters, col_name, dfname):\n",
    "    documents = df[col_name].dropna().tolist()\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_word_set)\n",
    "    idf_vector = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    kmeans=KMeans(n_clusters=num_clusters, random_state=42, n_init=10, max_iter=500)\n",
    "    clusters = kmeans.fit_predict(idf_vector.toarray())\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    cluster_docs = []\n",
    "    for cluster in range(num_clusters):\n",
    "        current_docs = []\n",
    "    \n",
    "        for i in range(len(df[col_name])):\n",
    "            if labels[i] == cluster:\n",
    "                current_docs.append(df.at[i, col_name])\n",
    "                \n",
    "        cluster_docs.append(current_docs)\n",
    "\n",
    "    vectorizers = []\n",
    "    for docs in cluster_docs:\n",
    "        vectorizers.append(vectorizer1.fit_transform(docs))\n",
    "\n",
    "    #plot wordcloud for each cluster\n",
    "\n",
    "    #create figure\n",
    "    plt.figure(figsize=(20,15), facecolor = None)\n",
    "    \n",
    "    #dynamic subplot layout\n",
    "    rows = int(np.ceil(num_clusters / 2))\n",
    "    cols = 2 if num_clusters > 1 else 1\n",
    "   \n",
    "    idx = 1\n",
    "    for v in vectorizers:\n",
    "        tfidf_scores = v.sum(axis=0).A1\n",
    "        \n",
    "        # Create dictionary of words and their TF-IDF scores\n",
    "        word_freq = dict(zip(feature_names, tfidf_scores))\n",
    "    \n",
    "        #visualize tf-idf frquencies using wordcloud\n",
    "        wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color ='white',\n",
    "                    stopwords = stopwords,\n",
    "                    min_font_size = 10).generate_from_frequencies(word_freq)\n",
    "        plt.subplot(rows, cols, idx)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(f\"Cluster {cluster}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad = 0)\n",
    "        plt.title('Cluster' + str(idx))\n",
    "        idx+=1\n",
    "        \n",
    "    plt.suptitle('K-Means Clustering - Top Clusters', fontsize=16, y=1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4d2d13-7a16-454c-b13f-43f58f7e67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xavie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\xavie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#load datasets \n",
    "news_excerpts = pd.read_excel('news_excerpts_parsed.xlsx')\n",
    "wikileaks = pd.read_excel('wikileaks_parsed.xlsx')\n",
    "\n",
    "#import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "stop_word_set = list(stopwords.words('english'))\n",
    "\n",
    "#Stem words \n",
    "# porter = PorterStemmer()\n",
    "# wikileaks['Stemmed'] = wikileaks['Text'].map(reduce(lambda x: stem_words(x)))\n",
    "    \n",
    "#displays visual representation of entity words\n",
    "#displacy.render(text1, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e727c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d4f9aa-8952-4abc-8f0d-82a18360bad4",
   "metadata": {},
   "source": [
    "# Tableau Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0ac6d7-71ac-4a21-9418-66599733cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Term  Count\n",
      "14312       said   1364\n",
      "15073  singapore    608\n",
      "18257       year    552\n",
      "1077        also    426\n",
      "17391         us    397\n",
      "          Term  Count\n",
      "226    airport    248\n",
      "1877  official    238\n",
      "2566     staff    154\n",
      "2099  pristina    136\n",
      "2846    vendor    126\n",
      "            Term  Tfidf Sum\n",
      "14312       said  39.050834\n",
      "15073  singapore  27.670553\n",
      "18257       year  23.424519\n",
      "17391         us  20.786353\n",
      "3360       china  20.559720\n",
      "          Term  Tfidf Sum\n",
      "226    airport   9.242965\n",
      "1877  official   7.972161\n",
      "2846    vendor   6.326134\n",
      "2566     staff   5.823963\n",
      "2099  pristina   5.638300\n"
     ]
    }
   ],
   "source": [
    "def get_word_count_df(df, stop_words):\n",
    "    #get word_count dict for wikileaks excerpts\n",
    "    count_vect = CountVectorizer(stop_words=stop_words)\n",
    "    \n",
    "    count_matrix = count_vect.fit_transform(df['Text'])\n",
    "    vocab = count_vect.vocabulary_\n",
    "    word_counts = count_matrix.sum(axis=0).A1\n",
    "    \n",
    "    #create dict of word counts\n",
    "    word_freq = dict(zip(count_vect.get_feature_names_out(), word_counts))\n",
    "    \n",
    "    count_df = pd.DataFrame(word_freq.items(), columns=['Term', 'Count'])\n",
    "    count_df = count_df.sort_values('Count', ascending=False)\n",
    "\n",
    "    return count_df\n",
    "\n",
    "def get_tfidf_count_df(df, stop_words):\n",
    "    #get word_count dict for wikileaks excerpts\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=stop_words)\n",
    "    \n",
    "    tfidf_matrix = tfidf_vect.fit_transform(df['Text'])\n",
    "    vocab = tfidf_vect.vocabulary_\n",
    "    tfidf_counts = tfidf_matrix.sum(axis=0).A1\n",
    "    \n",
    "    #create dict of word counts\n",
    "    tfidf_freq = dict(zip(tfidf_vect.get_feature_names_out(), tfidf_counts))\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(tfidf_freq.items(), columns=['Term', 'Tfidf Sum'])\n",
    "    tfidf_df = tfidf_df.sort_values('Tfidf Sum', ascending=False)\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "\n",
    "wiki_count_df = get_word_count_df(wikileaks, stop_word_set)\n",
    "news_count_df = get_word_count_df(news_excerpts, stop_word_set)\n",
    "wiki_tfidf_df = get_tfidf_count_df(wikileaks, stop_word_set)\n",
    "news_tfidf_df = get_tfidf_count_df(news_excerpts, stop_word_set)\n",
    "print(news_count_df.head())\n",
    "print(wiki_count_df.head())\n",
    "print(news_tfidf_df.head())\n",
    "print(wiki_tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d9730c-2b08-42df-a760-7c5d7efff524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Term  Count  Tfidf Sum\n",
      "0               said   1364  39.050834\n",
      "1          singapore    608  27.670553\n",
      "2               year    552  23.424519\n",
      "3               also    426  18.493828\n",
      "4                 us    397  20.786353\n",
      "...              ...    ...        ...\n",
      "18411      insolvent      1   0.169491\n",
      "18412    inspections      1   0.148635\n",
      "18413  inspirational      1   0.119247\n",
      "18414       inspires      1   0.162357\n",
      "18415            蘇姿丰      1   0.110509\n",
      "\n",
      "[18416 rows x 3 columns]                 Term  Count  Tfidf Sum\n",
      "0            airport    248   9.242965\n",
      "1           official    238   7.972161\n",
      "2              staff    154   5.823963\n",
      "3           pristina    136   5.638300\n",
      "4             vendor    126   6.326134\n",
      "...              ...    ...        ...\n",
      "2941           abuse      1   0.118810\n",
      "2942       intervene      1   0.091820\n",
      "2943  interpretation      1   0.129827\n",
      "2944         abusing      1   0.111075\n",
      "2945         zealand      1   0.075207\n",
      "\n",
      "[2946 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#combine both dataframes\n",
    "news_term_df = news_count_df.merge(news_tfidf_df, on='Term')\n",
    "wiki_term_df = wiki_count_df.merge(wiki_tfidf_df, on='Term')\n",
    "print(news_term_df, wiki_term_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df74605-ec11-4fe6-9d9b-ce723a342b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write both dfs to new excel file\n",
    "with pd.ExcelWriter(\"./wikileaks_term_counts.xlsx\") as writer:\n",
    "    wiki_term_df.to_excel(writer)\n",
    "    \n",
    "with pd.ExcelWriter(\"./news_term_counts.xlsx\") as writer:\n",
    "    news_term_df.to_excel(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3005f0-349c-4feb-b34e-52e18c1a36df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m     doc \u001b[38;5;241m=\u001b[39m NER(text)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m      9\u001b[0m         {\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: ent\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments\n\u001b[0;32m     14\u001b[0m     ]\n\u001b[1;32m---> 16\u001b[0m NER \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# process labels for each entry, store as DF col\u001b[39;00m\n\u001b[0;32m     19\u001b[0m news_excerpts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m news_excerpts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_entities(NER, x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "# wikileaks['Words'] = wikileaks['Text'].map(lambda x: word_tokenize(x))\n",
    "# wikileaks['Filtered'] = wikileaks['Words'].map(lambda x: [w for w in x if not w.lower() in stop_word_set])\n",
    "\n",
    "def extract_entities(NER, text):\n",
    "    \"\"\"extract NERs from selected text and return it as a list of dictionaries\"\"\"\n",
    "    doc = NER(text)\n",
    "    return [\n",
    "        {\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "        }\n",
    "        for ent in doc.ents\n",
    "    ]\n",
    "    \n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process labels for each entry, store as DF col\n",
    "news_excerpts['Labels'] = news_excerpts['Text'].map(lambda x: extract_entities(NER, x))\n",
    "wikileaks['Labels'] = wikileaks['Text'].map(lambda x: extract_entities(NER, x))\n",
    "news_count_df['Labels'] = news_count_df['Term'].map(lambda x: extract_entities(NER, x))\n",
    "print(news_count_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d2a2f2-99a0-40b1-9703-c77d6f8ca35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write counts of labels to new excel file\n",
    "with pd.ExcelWriter(\"./news_label_counts.xlsx\") as writer:\n",
    "    news_count_df.to_excel(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aff00e8-4463-4c22-a6b7-37f1504784f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wikileaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m label_types:\n\u001b[0;32m      5\u001b[0m         df[label] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m excerpt: [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m excerpt \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m label])\n\u001b[1;32m----> 7\u001b[0m process_labels(wikileaks)\n\u001b[0;32m      8\u001b[0m process_labels(news_excerpts)\n\u001b[0;32m     10\u001b[0m news_excerpts\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wikileaks' is not defined"
     ]
    }
   ],
   "source": [
    "def process_labels(df):\n",
    "    \"\"\"extract each label and store as new col in dataframe\"\"\"\n",
    "    label_types = ['PERSON', 'ORG', 'GPE', 'DATE', 'MONEY', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "    for label in label_types:\n",
    "        df[label] = df['Labels'].map(lambda excerpt: [x for x in excerpt if x.get('label') == label])\n",
    "\n",
    "process_labels(wikileaks)\n",
    "process_labels(news_excerpts)\n",
    "\n",
    "news_excerpts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94877737-88c9-4384-8fdd-1d34e7f31d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write processed df to a new excel sheet for tableau processing\n",
    "with pd.ExcelWriter(\"./news_excerpts_labels.xlsx\") as writer:\n",
    "    news_excerpts.to_excel(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e0fa810-fcd2-4e62-b0ed-ec4b84e5a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1509, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Persons</th>\n",
       "      <th>Organisations</th>\n",
       "      <th>Geopolitical Entities</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Money</th>\n",
       "      <th>Location</th>\n",
       "      <th>Product</th>\n",
       "      <th>Events</th>\n",
       "      <th>Artwork</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'text': 'Mara-Louise Anzalone', 'label': 'PE...</td>\n",
       "      <td>[{'text': 'National Labor Relations Board', 'l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'Thursday', 'label': 'DATE', 'start'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'text': 'Su Wenqiang', 'label': 'PERSON', 's...</td>\n",
       "      <td>[{'text': 'Bukit Timah', 'label': 'ORG', 'star...</td>\n",
       "      <td>[{'text': 'Singapore', 'label': 'GPE', 'start'...</td>\n",
       "      <td>[{'text': \"13 months'\", 'label': 'DATE', 'star...</td>\n",
       "      <td>[{'text': 'More than S$3 billion', 'label': 'M...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'Meta', 'label': 'ORG', 'start': 0, ...</td>\n",
       "      <td>[{'text': 'the United States', 'label': 'GPE',...</td>\n",
       "      <td>[{'text': 'Monday', 'label': 'DATE', 'start': ...</td>\n",
       "      <td>[{'text': '€1.2 billion', 'label': 'MONEY', 's...</td>\n",
       "      <td>[{'text': 'Europe', 'label': 'LOC', 'start': 4...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'Facebook (FB', 'label': 'WORK_OF_AR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'text': 'Zhang Ruijin', 'label': 'PERSON', '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'SINGAPORE', 'label': 'GPE', 'start'...</td>\n",
       "      <td>[{'text': '45-year-old', 'label': 'DATE', 'sta...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[{'text': 'The Department of Education', 'labe...</td>\n",
       "      <td>[{'text': 'Virginia', 'label': 'GPE', 'start':...</td>\n",
       "      <td>[{'text': 'Tuesday', 'label': 'DATE', 'start':...</td>\n",
       "      <td>[{'text': 'a record $14 million', 'label': 'MO...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Persons  \\\n",
       "0  [{'text': 'Mara-Louise Anzalone', 'label': 'PE...   \n",
       "1  [{'text': 'Su Wenqiang', 'label': 'PERSON', 's...   \n",
       "2                                                 []   \n",
       "3  [{'text': 'Zhang Ruijin', 'label': 'PERSON', '...   \n",
       "4                                                 []   \n",
       "\n",
       "                                       Organisations  \\\n",
       "0  [{'text': 'National Labor Relations Board', 'l...   \n",
       "1  [{'text': 'Bukit Timah', 'label': 'ORG', 'star...   \n",
       "2  [{'text': 'Meta', 'label': 'ORG', 'start': 0, ...   \n",
       "3                                                 []   \n",
       "4  [{'text': 'The Department of Education', 'labe...   \n",
       "\n",
       "                               Geopolitical Entities  \\\n",
       "0                                                 []   \n",
       "1  [{'text': 'Singapore', 'label': 'GPE', 'start'...   \n",
       "2  [{'text': 'the United States', 'label': 'GPE',...   \n",
       "3  [{'text': 'SINGAPORE', 'label': 'GPE', 'start'...   \n",
       "4  [{'text': 'Virginia', 'label': 'GPE', 'start':...   \n",
       "\n",
       "                                               Dates  \\\n",
       "0  [{'text': 'Thursday', 'label': 'DATE', 'start'...   \n",
       "1  [{'text': \"13 months'\", 'label': 'DATE', 'star...   \n",
       "2  [{'text': 'Monday', 'label': 'DATE', 'start': ...   \n",
       "3  [{'text': '45-year-old', 'label': 'DATE', 'sta...   \n",
       "4  [{'text': 'Tuesday', 'label': 'DATE', 'start':...   \n",
       "\n",
       "                                               Money  \\\n",
       "0                                                 []   \n",
       "1  [{'text': 'More than S$3 billion', 'label': 'M...   \n",
       "2  [{'text': '€1.2 billion', 'label': 'MONEY', 's...   \n",
       "3                                                 []   \n",
       "4  [{'text': 'a record $14 million', 'label': 'MO...   \n",
       "\n",
       "                                            Location Product Events  \\\n",
       "0                                                 []      []     []   \n",
       "1                                                 []      []     []   \n",
       "2  [{'text': 'Europe', 'label': 'LOC', 'start': 4...      []     []   \n",
       "3                                                 []      []     []   \n",
       "4                                                 []      []     []   \n",
       "\n",
       "                                             Artwork  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [{'text': 'Facebook (FB', 'label': 'WORK_OF_AR...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select only the extracted labels\n",
    "print(news_excerpts.shape)\n",
    "news_labels = news_excerpts.loc[:,'Persons':'Artwork']\n",
    "news_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855587c-065b-4cb2-a82c-3359155f2fdd",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77a2be-8d83-416b-a71c-9d91c4606054",
   "metadata": {},
   "source": [
    "cluster_visualise_scatter(wikileaks, 6, 'Text', 'wikileaks')\n",
    "cluster_visualise_scatter(news_excerpts, 6, 'Text', 'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65055804-91a9-48bf-9a07-e5985a4ea504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavie\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m visualize_wordcloud(wikileaks, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwikileaks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m visualize_wordcloud(news_excerpts, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 55\u001b[0m, in \u001b[0;36mvisualize_wordcloud\u001b[1;34m(df, num_clusters, col_name, dfname)\u001b[0m\n\u001b[0;32m     53\u001b[0m vectorizers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m docs \u001b[38;5;129;01min\u001b[39;00m cluster_docs:\n\u001b[1;32m---> 55\u001b[0m     vectorizers\u001b[38;5;241m.\u001b[39mappend(vectorizer1\u001b[38;5;241m.\u001b[39mfit_transform(docs))\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#plot wordcloud for each cluster\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#create figure\u001b[39;00m\n\u001b[0;32m     60\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m15\u001b[39m), facecolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer1' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_wordcloud(wikileaks, 6, 'Text', 'wikileaks')\n",
    "visualize_wordcloud(news_excerpts, 6, 'Text', 'news')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e551c-66ce-4f9e-a5c4-e0dd0135ca91",
   "metadata": {},
   "source": [
    "# Network Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b579c-b118-4518-8448-5cd2b790def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_entity_graph(excerpt_entities):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i, entities in enumerate(excerpt_entities):\n",
    "        for entity in entities:\n",
    "            #Add node w metadata\n",
    "            G.add_node((entity['text'],\n",
    "                       entity['label'],\n",
    "                       i))\n",
    "            \n",
    "        #connect entities within the same excerpt\n",
    "        for other_entity in entities:\n",
    "            if entity != other_entity:\n",
    "                G.add_edge(entity['text'], other_entity['text'], weight=1)\n",
    "                \n",
    "    return G\n",
    "\n",
    "def build_relationship_dict(excerpt_entities):\n",
    "    count_dict = {}\n",
    "    \n",
    "    for i, entities in enumerate(excerpt_entities):\n",
    "        for entity in entities   \n",
    "            for other_entity in entities:\n",
    "                if entity['text'] != other_entity['text']:\n",
    "                    entity_pair = tuple(sorted((entity['text'], other_entity['text'])))\n",
    "                    occ_count = count_dict.get(entity_pair, 0)\n",
    "                    occ_count += 1\n",
    "                    count_dict.update({entity_pair: occ_count})\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da72e0-e62b-49c9-a0f4-359651a36d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create relationship_dict and sort it\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "relationship_dict = build_relationship_dict(wikileaks['Labels'])\n",
    "\n",
    "# keys = list(relationship_dict.keys())\n",
    "# values = list(relationship_dict.values())\n",
    "# values.sort(reverse=True)\n",
    "# sorted_dict = {keys[i]: values[i] for i in values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89c8bc-64de-4dbc-bb6b-3cd97d5e253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique terms\n",
    "unique_terms = []\n",
    "for i in wikileaks['Labels']:\n",
    "    for j in i:\n",
    "        unique_terms.append(j['text'])\n",
    "unique_terms = set(unique_terms)\n",
    "print(unique_terms)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c19153-1a26-4813-b1b7-4a2352e0040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_relationship_graph(weighted_dict, key_set):\n",
    "    G = nx.Graph()\n",
    "    # create nodes\n",
    "    for key in key_set:\n",
    "        G.add_node(key)\n",
    "\n",
    "    #create weighted edges\n",
    "    #connect entities within the same excerpt\n",
    "    for key in weighted_dict.keys():\n",
    "            G.add_edge(key[0], key[1], weight=weighted_dict.get(key))\n",
    "    return G\n",
    "\n",
    "relationship_graph = build_relationship_graph(relationship_dict, unique_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81034fd0-de6a-41ff-a9ca-f4347f9f0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise graph\n",
    "import matplotlib.pyplot as plt\n",
    "nx.draw(relationship_graph, with_labels=True)\n",
    "plt.show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe65d1b-75f0-4697-ac67-86b14fd91091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataloader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480bbd-7b97-4114-ac8b-100c27ee730f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
